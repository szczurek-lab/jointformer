{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.configs.trainer import TrainerConfig\n",
    "\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.trainers.trainer import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\"GPU not detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "REPOSITORY_DIR = '/home/adamizdebski/projects/jointformer'\n",
    "DATA_DIR = '/home/adamizdebski/files/data'\n",
    "OUTPUT_DIR = '/home/adamizdebski/files/jointformer/results/chemberta2/moleculenet'\n",
    "\n",
    "PATH_TO_DATASET_CONFIG   = '/home/adamizdebski/projects/jointformer/configs/datasets/guacamol/qed'\n",
    "PATH_TO_TOKENIZER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/tokenizers/chemberta'\n",
    "PATH_TO_MODEL_CONFIG = '/home/adamizdebski/projects/jointformer/configs/models/chemberta'\n",
    "PATH_TO_TRAINER_CONFIG = '/home/adamizdebski/projects/jointformer/configs/trainers/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(REPOSITORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Datsaset\n",
    "\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_TOKENIZER_CONFIG)\n",
    "\n",
    "train_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='train')\n",
    "val_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='val')\n",
    "test_dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='test')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig.from_config_file(PATH_TO_MODEL_CONFIG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config.pretrained_filepath = 'DeepChem/ChemBERTa-10M-MTR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MTR were not used when initializing RobertaForRegression: ['norm_mean', 'norm_std']\n",
      "- This IS expected if you are initializing RobertaForRegression from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForRegression from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForRegression were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized because the shapes did not match:\n",
      "- regression.out_proj.weight: found shape torch.Size([199, 384]) in the checkpoint and torch.Size([1, 384]) in the model instantiated\n",
      "- regression.out_proj.bias: found shape torch.Size([199]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_config(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForRegression(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "      (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.144, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.109, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.144, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.144, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regression): RobertaRegressionHead(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): Dropout(p=0.144, inplace=False)\n",
       "    (out_proj): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_encoder = model.to_smiles_encoder(tokenizer, batch_size=2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding samples: 100%|██████████| 8/8 [00:05<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "data = test_dataset.data[:16]\n",
    "smiles_encoded = smiles_encoder.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 384)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'chemberta.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_pretrained('chemberta.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForRegression(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(600, 384, padding_idx=1)\n",
       "      (position_embeddings): Embedding(515, 384, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.144, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-2): 3 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.109, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.144, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=464, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=464, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.144, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (regression): RobertaRegressionHead(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): Dropout(p=0.144, inplace=False)\n",
       "    (out_proj): Linear(in_features=384, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "roberta_config = RobertaConfig.from_pretrained(model_config.pretrained_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"architectures\": [\n",
       "    \"RobertaForRegression\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.109,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.144,\n",
       "  \"hidden_size\": 384,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\",\n",
       "    \"13\": \"LABEL_13\",\n",
       "    \"14\": \"LABEL_14\",\n",
       "    \"15\": \"LABEL_15\",\n",
       "    \"16\": \"LABEL_16\",\n",
       "    \"17\": \"LABEL_17\",\n",
       "    \"18\": \"LABEL_18\",\n",
       "    \"19\": \"LABEL_19\",\n",
       "    \"20\": \"LABEL_20\",\n",
       "    \"21\": \"LABEL_21\",\n",
       "    \"22\": \"LABEL_22\",\n",
       "    \"23\": \"LABEL_23\",\n",
       "    \"24\": \"LABEL_24\",\n",
       "    \"25\": \"LABEL_25\",\n",
       "    \"26\": \"LABEL_26\",\n",
       "    \"27\": \"LABEL_27\",\n",
       "    \"28\": \"LABEL_28\",\n",
       "    \"29\": \"LABEL_29\",\n",
       "    \"30\": \"LABEL_30\",\n",
       "    \"31\": \"LABEL_31\",\n",
       "    \"32\": \"LABEL_32\",\n",
       "    \"33\": \"LABEL_33\",\n",
       "    \"34\": \"LABEL_34\",\n",
       "    \"35\": \"LABEL_35\",\n",
       "    \"36\": \"LABEL_36\",\n",
       "    \"37\": \"LABEL_37\",\n",
       "    \"38\": \"LABEL_38\",\n",
       "    \"39\": \"LABEL_39\",\n",
       "    \"40\": \"LABEL_40\",\n",
       "    \"41\": \"LABEL_41\",\n",
       "    \"42\": \"LABEL_42\",\n",
       "    \"43\": \"LABEL_43\",\n",
       "    \"44\": \"LABEL_44\",\n",
       "    \"45\": \"LABEL_45\",\n",
       "    \"46\": \"LABEL_46\",\n",
       "    \"47\": \"LABEL_47\",\n",
       "    \"48\": \"LABEL_48\",\n",
       "    \"49\": \"LABEL_49\",\n",
       "    \"50\": \"LABEL_50\",\n",
       "    \"51\": \"LABEL_51\",\n",
       "    \"52\": \"LABEL_52\",\n",
       "    \"53\": \"LABEL_53\",\n",
       "    \"54\": \"LABEL_54\",\n",
       "    \"55\": \"LABEL_55\",\n",
       "    \"56\": \"LABEL_56\",\n",
       "    \"57\": \"LABEL_57\",\n",
       "    \"58\": \"LABEL_58\",\n",
       "    \"59\": \"LABEL_59\",\n",
       "    \"60\": \"LABEL_60\",\n",
       "    \"61\": \"LABEL_61\",\n",
       "    \"62\": \"LABEL_62\",\n",
       "    \"63\": \"LABEL_63\",\n",
       "    \"64\": \"LABEL_64\",\n",
       "    \"65\": \"LABEL_65\",\n",
       "    \"66\": \"LABEL_66\",\n",
       "    \"67\": \"LABEL_67\",\n",
       "    \"68\": \"LABEL_68\",\n",
       "    \"69\": \"LABEL_69\",\n",
       "    \"70\": \"LABEL_70\",\n",
       "    \"71\": \"LABEL_71\",\n",
       "    \"72\": \"LABEL_72\",\n",
       "    \"73\": \"LABEL_73\",\n",
       "    \"74\": \"LABEL_74\",\n",
       "    \"75\": \"LABEL_75\",\n",
       "    \"76\": \"LABEL_76\",\n",
       "    \"77\": \"LABEL_77\",\n",
       "    \"78\": \"LABEL_78\",\n",
       "    \"79\": \"LABEL_79\",\n",
       "    \"80\": \"LABEL_80\",\n",
       "    \"81\": \"LABEL_81\",\n",
       "    \"82\": \"LABEL_82\",\n",
       "    \"83\": \"LABEL_83\",\n",
       "    \"84\": \"LABEL_84\",\n",
       "    \"85\": \"LABEL_85\",\n",
       "    \"86\": \"LABEL_86\",\n",
       "    \"87\": \"LABEL_87\",\n",
       "    \"88\": \"LABEL_88\",\n",
       "    \"89\": \"LABEL_89\",\n",
       "    \"90\": \"LABEL_90\",\n",
       "    \"91\": \"LABEL_91\",\n",
       "    \"92\": \"LABEL_92\",\n",
       "    \"93\": \"LABEL_93\",\n",
       "    \"94\": \"LABEL_94\",\n",
       "    \"95\": \"LABEL_95\",\n",
       "    \"96\": \"LABEL_96\",\n",
       "    \"97\": \"LABEL_97\",\n",
       "    \"98\": \"LABEL_98\",\n",
       "    \"99\": \"LABEL_99\",\n",
       "    \"100\": \"LABEL_100\",\n",
       "    \"101\": \"LABEL_101\",\n",
       "    \"102\": \"LABEL_102\",\n",
       "    \"103\": \"LABEL_103\",\n",
       "    \"104\": \"LABEL_104\",\n",
       "    \"105\": \"LABEL_105\",\n",
       "    \"106\": \"LABEL_106\",\n",
       "    \"107\": \"LABEL_107\",\n",
       "    \"108\": \"LABEL_108\",\n",
       "    \"109\": \"LABEL_109\",\n",
       "    \"110\": \"LABEL_110\",\n",
       "    \"111\": \"LABEL_111\",\n",
       "    \"112\": \"LABEL_112\",\n",
       "    \"113\": \"LABEL_113\",\n",
       "    \"114\": \"LABEL_114\",\n",
       "    \"115\": \"LABEL_115\",\n",
       "    \"116\": \"LABEL_116\",\n",
       "    \"117\": \"LABEL_117\",\n",
       "    \"118\": \"LABEL_118\",\n",
       "    \"119\": \"LABEL_119\",\n",
       "    \"120\": \"LABEL_120\",\n",
       "    \"121\": \"LABEL_121\",\n",
       "    \"122\": \"LABEL_122\",\n",
       "    \"123\": \"LABEL_123\",\n",
       "    \"124\": \"LABEL_124\",\n",
       "    \"125\": \"LABEL_125\",\n",
       "    \"126\": \"LABEL_126\",\n",
       "    \"127\": \"LABEL_127\",\n",
       "    \"128\": \"LABEL_128\",\n",
       "    \"129\": \"LABEL_129\",\n",
       "    \"130\": \"LABEL_130\",\n",
       "    \"131\": \"LABEL_131\",\n",
       "    \"132\": \"LABEL_132\",\n",
       "    \"133\": \"LABEL_133\",\n",
       "    \"134\": \"LABEL_134\",\n",
       "    \"135\": \"LABEL_135\",\n",
       "    \"136\": \"LABEL_136\",\n",
       "    \"137\": \"LABEL_137\",\n",
       "    \"138\": \"LABEL_138\",\n",
       "    \"139\": \"LABEL_139\",\n",
       "    \"140\": \"LABEL_140\",\n",
       "    \"141\": \"LABEL_141\",\n",
       "    \"142\": \"LABEL_142\",\n",
       "    \"143\": \"LABEL_143\",\n",
       "    \"144\": \"LABEL_144\",\n",
       "    \"145\": \"LABEL_145\",\n",
       "    \"146\": \"LABEL_146\",\n",
       "    \"147\": \"LABEL_147\",\n",
       "    \"148\": \"LABEL_148\",\n",
       "    \"149\": \"LABEL_149\",\n",
       "    \"150\": \"LABEL_150\",\n",
       "    \"151\": \"LABEL_151\",\n",
       "    \"152\": \"LABEL_152\",\n",
       "    \"153\": \"LABEL_153\",\n",
       "    \"154\": \"LABEL_154\",\n",
       "    \"155\": \"LABEL_155\",\n",
       "    \"156\": \"LABEL_156\",\n",
       "    \"157\": \"LABEL_157\",\n",
       "    \"158\": \"LABEL_158\",\n",
       "    \"159\": \"LABEL_159\",\n",
       "    \"160\": \"LABEL_160\",\n",
       "    \"161\": \"LABEL_161\",\n",
       "    \"162\": \"LABEL_162\",\n",
       "    \"163\": \"LABEL_163\",\n",
       "    \"164\": \"LABEL_164\",\n",
       "    \"165\": \"LABEL_165\",\n",
       "    \"166\": \"LABEL_166\",\n",
       "    \"167\": \"LABEL_167\",\n",
       "    \"168\": \"LABEL_168\",\n",
       "    \"169\": \"LABEL_169\",\n",
       "    \"170\": \"LABEL_170\",\n",
       "    \"171\": \"LABEL_171\",\n",
       "    \"172\": \"LABEL_172\",\n",
       "    \"173\": \"LABEL_173\",\n",
       "    \"174\": \"LABEL_174\",\n",
       "    \"175\": \"LABEL_175\",\n",
       "    \"176\": \"LABEL_176\",\n",
       "    \"177\": \"LABEL_177\",\n",
       "    \"178\": \"LABEL_178\",\n",
       "    \"179\": \"LABEL_179\",\n",
       "    \"180\": \"LABEL_180\",\n",
       "    \"181\": \"LABEL_181\",\n",
       "    \"182\": \"LABEL_182\",\n",
       "    \"183\": \"LABEL_183\",\n",
       "    \"184\": \"LABEL_184\",\n",
       "    \"185\": \"LABEL_185\",\n",
       "    \"186\": \"LABEL_186\",\n",
       "    \"187\": \"LABEL_187\",\n",
       "    \"188\": \"LABEL_188\",\n",
       "    \"189\": \"LABEL_189\",\n",
       "    \"190\": \"LABEL_190\",\n",
       "    \"191\": \"LABEL_191\",\n",
       "    \"192\": \"LABEL_192\",\n",
       "    \"193\": \"LABEL_193\",\n",
       "    \"194\": \"LABEL_194\",\n",
       "    \"195\": \"LABEL_195\",\n",
       "    \"196\": \"LABEL_196\",\n",
       "    \"197\": \"LABEL_197\",\n",
       "    \"198\": \"LABEL_198\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 464,\n",
       "  \"is_gpu\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_100\": 100,\n",
       "    \"LABEL_101\": 101,\n",
       "    \"LABEL_102\": 102,\n",
       "    \"LABEL_103\": 103,\n",
       "    \"LABEL_104\": 104,\n",
       "    \"LABEL_105\": 105,\n",
       "    \"LABEL_106\": 106,\n",
       "    \"LABEL_107\": 107,\n",
       "    \"LABEL_108\": 108,\n",
       "    \"LABEL_109\": 109,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_110\": 110,\n",
       "    \"LABEL_111\": 111,\n",
       "    \"LABEL_112\": 112,\n",
       "    \"LABEL_113\": 113,\n",
       "    \"LABEL_114\": 114,\n",
       "    \"LABEL_115\": 115,\n",
       "    \"LABEL_116\": 116,\n",
       "    \"LABEL_117\": 117,\n",
       "    \"LABEL_118\": 118,\n",
       "    \"LABEL_119\": 119,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_120\": 120,\n",
       "    \"LABEL_121\": 121,\n",
       "    \"LABEL_122\": 122,\n",
       "    \"LABEL_123\": 123,\n",
       "    \"LABEL_124\": 124,\n",
       "    \"LABEL_125\": 125,\n",
       "    \"LABEL_126\": 126,\n",
       "    \"LABEL_127\": 127,\n",
       "    \"LABEL_128\": 128,\n",
       "    \"LABEL_129\": 129,\n",
       "    \"LABEL_13\": 13,\n",
       "    \"LABEL_130\": 130,\n",
       "    \"LABEL_131\": 131,\n",
       "    \"LABEL_132\": 132,\n",
       "    \"LABEL_133\": 133,\n",
       "    \"LABEL_134\": 134,\n",
       "    \"LABEL_135\": 135,\n",
       "    \"LABEL_136\": 136,\n",
       "    \"LABEL_137\": 137,\n",
       "    \"LABEL_138\": 138,\n",
       "    \"LABEL_139\": 139,\n",
       "    \"LABEL_14\": 14,\n",
       "    \"LABEL_140\": 140,\n",
       "    \"LABEL_141\": 141,\n",
       "    \"LABEL_142\": 142,\n",
       "    \"LABEL_143\": 143,\n",
       "    \"LABEL_144\": 144,\n",
       "    \"LABEL_145\": 145,\n",
       "    \"LABEL_146\": 146,\n",
       "    \"LABEL_147\": 147,\n",
       "    \"LABEL_148\": 148,\n",
       "    \"LABEL_149\": 149,\n",
       "    \"LABEL_15\": 15,\n",
       "    \"LABEL_150\": 150,\n",
       "    \"LABEL_151\": 151,\n",
       "    \"LABEL_152\": 152,\n",
       "    \"LABEL_153\": 153,\n",
       "    \"LABEL_154\": 154,\n",
       "    \"LABEL_155\": 155,\n",
       "    \"LABEL_156\": 156,\n",
       "    \"LABEL_157\": 157,\n",
       "    \"LABEL_158\": 158,\n",
       "    \"LABEL_159\": 159,\n",
       "    \"LABEL_16\": 16,\n",
       "    \"LABEL_160\": 160,\n",
       "    \"LABEL_161\": 161,\n",
       "    \"LABEL_162\": 162,\n",
       "    \"LABEL_163\": 163,\n",
       "    \"LABEL_164\": 164,\n",
       "    \"LABEL_165\": 165,\n",
       "    \"LABEL_166\": 166,\n",
       "    \"LABEL_167\": 167,\n",
       "    \"LABEL_168\": 168,\n",
       "    \"LABEL_169\": 169,\n",
       "    \"LABEL_17\": 17,\n",
       "    \"LABEL_170\": 170,\n",
       "    \"LABEL_171\": 171,\n",
       "    \"LABEL_172\": 172,\n",
       "    \"LABEL_173\": 173,\n",
       "    \"LABEL_174\": 174,\n",
       "    \"LABEL_175\": 175,\n",
       "    \"LABEL_176\": 176,\n",
       "    \"LABEL_177\": 177,\n",
       "    \"LABEL_178\": 178,\n",
       "    \"LABEL_179\": 179,\n",
       "    \"LABEL_18\": 18,\n",
       "    \"LABEL_180\": 180,\n",
       "    \"LABEL_181\": 181,\n",
       "    \"LABEL_182\": 182,\n",
       "    \"LABEL_183\": 183,\n",
       "    \"LABEL_184\": 184,\n",
       "    \"LABEL_185\": 185,\n",
       "    \"LABEL_186\": 186,\n",
       "    \"LABEL_187\": 187,\n",
       "    \"LABEL_188\": 188,\n",
       "    \"LABEL_189\": 189,\n",
       "    \"LABEL_19\": 19,\n",
       "    \"LABEL_190\": 190,\n",
       "    \"LABEL_191\": 191,\n",
       "    \"LABEL_192\": 192,\n",
       "    \"LABEL_193\": 193,\n",
       "    \"LABEL_194\": 194,\n",
       "    \"LABEL_195\": 195,\n",
       "    \"LABEL_196\": 196,\n",
       "    \"LABEL_197\": 197,\n",
       "    \"LABEL_198\": 198,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_20\": 20,\n",
       "    \"LABEL_21\": 21,\n",
       "    \"LABEL_22\": 22,\n",
       "    \"LABEL_23\": 23,\n",
       "    \"LABEL_24\": 24,\n",
       "    \"LABEL_25\": 25,\n",
       "    \"LABEL_26\": 26,\n",
       "    \"LABEL_27\": 27,\n",
       "    \"LABEL_28\": 28,\n",
       "    \"LABEL_29\": 29,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_30\": 30,\n",
       "    \"LABEL_31\": 31,\n",
       "    \"LABEL_32\": 32,\n",
       "    \"LABEL_33\": 33,\n",
       "    \"LABEL_34\": 34,\n",
       "    \"LABEL_35\": 35,\n",
       "    \"LABEL_36\": 36,\n",
       "    \"LABEL_37\": 37,\n",
       "    \"LABEL_38\": 38,\n",
       "    \"LABEL_39\": 39,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_40\": 40,\n",
       "    \"LABEL_41\": 41,\n",
       "    \"LABEL_42\": 42,\n",
       "    \"LABEL_43\": 43,\n",
       "    \"LABEL_44\": 44,\n",
       "    \"LABEL_45\": 45,\n",
       "    \"LABEL_46\": 46,\n",
       "    \"LABEL_47\": 47,\n",
       "    \"LABEL_48\": 48,\n",
       "    \"LABEL_49\": 49,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_50\": 50,\n",
       "    \"LABEL_51\": 51,\n",
       "    \"LABEL_52\": 52,\n",
       "    \"LABEL_53\": 53,\n",
       "    \"LABEL_54\": 54,\n",
       "    \"LABEL_55\": 55,\n",
       "    \"LABEL_56\": 56,\n",
       "    \"LABEL_57\": 57,\n",
       "    \"LABEL_58\": 58,\n",
       "    \"LABEL_59\": 59,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_60\": 60,\n",
       "    \"LABEL_61\": 61,\n",
       "    \"LABEL_62\": 62,\n",
       "    \"LABEL_63\": 63,\n",
       "    \"LABEL_64\": 64,\n",
       "    \"LABEL_65\": 65,\n",
       "    \"LABEL_66\": 66,\n",
       "    \"LABEL_67\": 67,\n",
       "    \"LABEL_68\": 68,\n",
       "    \"LABEL_69\": 69,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_70\": 70,\n",
       "    \"LABEL_71\": 71,\n",
       "    \"LABEL_72\": 72,\n",
       "    \"LABEL_73\": 73,\n",
       "    \"LABEL_74\": 74,\n",
       "    \"LABEL_75\": 75,\n",
       "    \"LABEL_76\": 76,\n",
       "    \"LABEL_77\": 77,\n",
       "    \"LABEL_78\": 78,\n",
       "    \"LABEL_79\": 79,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_80\": 80,\n",
       "    \"LABEL_81\": 81,\n",
       "    \"LABEL_82\": 82,\n",
       "    \"LABEL_83\": 83,\n",
       "    \"LABEL_84\": 84,\n",
       "    \"LABEL_85\": 85,\n",
       "    \"LABEL_86\": 86,\n",
       "    \"LABEL_87\": 87,\n",
       "    \"LABEL_88\": 88,\n",
       "    \"LABEL_89\": 89,\n",
       "    \"LABEL_9\": 9,\n",
       "    \"LABEL_90\": 90,\n",
       "    \"LABEL_91\": 91,\n",
       "    \"LABEL_92\": 92,\n",
       "    \"LABEL_93\": 93,\n",
       "    \"LABEL_94\": 94,\n",
       "    \"LABEL_95\": 95,\n",
       "    \"LABEL_96\": 96,\n",
       "    \"LABEL_97\": 97,\n",
       "    \"LABEL_98\": 98,\n",
       "    \"LABEL_99\": 99\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 515,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"norm_mean\": [\n",
       "    11.199569164274653,\n",
       "    -0.9728601944583675,\n",
       "    11.199595401578872,\n",
       "    0.1914454376660732,\n",
       "    0.608589373135307,\n",
       "    365.064017672,\n",
       "    342.24912812000014,\n",
       "    364.6033136038417,\n",
       "    134.06547,\n",
       "    0.004249,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    1.1861084842221647,\n",
       "    1.890967178564785,\n",
       "    2.519587985439997,\n",
       "    2.0112818114267816,\n",
       "    795.5621221754437,\n",
       "    18.14439203724506,\n",
       "    14.536240385432393,\n",
       "    15.215140271072487,\n",
       "    12.068994414289726,\n",
       "    8.453657900068215,\n",
       "    9.114162139055054,\n",
       "    6.434168605708085,\n",
       "    7.215103879809845,\n",
       "    4.436200487997215,\n",
       "    5.109730699855831,\n",
       "    3.055231525907226,\n",
       "    3.6252747118486264,\n",
       "    -2.202564923376624,\n",
       "    18.195385007867852,\n",
       "    7.9706993589944775,\n",
       "    4.5379164631837545,\n",
       "    150.95250337667272,\n",
       "    13.184208966483704,\n",
       "    8.814008658052902,\n",
       "    3.8191839078987306,\n",
       "    3.4969386790830774,\n",
       "    2.9222201316693712,\n",
       "    2.644444123964607,\n",
       "    6.408740449956927,\n",
       "    4.95314480536345,\n",
       "    2.6263770771853108,\n",
       "    2.4113616526384853,\n",
       "    26.24052195128434,\n",
       "    37.102909834641714,\n",
       "    19.89943953042712,\n",
       "    16.353848799228413,\n",
       "    15.638332143998122,\n",
       "    21.706094849865753,\n",
       "    0.28727529762970366,\n",
       "    8.054432014422119,\n",
       "    3.2648099385428853,\n",
       "    32.629006626588726,\n",
       "    16.26551059790217,\n",
       "    47.70605007162041,\n",
       "    0.0,\n",
       "    5.325837027308287,\n",
       "    9.698460925314944,\n",
       "    5.573601891254677,\n",
       "    2.581492771453006,\n",
       "    7.3124961943884665,\n",
       "    33.07539073817076,\n",
       "    10.718462271839512,\n",
       "    6.99277406210818,\n",
       "    31.684923475431933,\n",
       "    36.92162447084414,\n",
       "    1.2074202610211657,\n",
       "    5.110701506051421,\n",
       "    0.0,\n",
       "    71.04050338999998,\n",
       "    9.57750975344203,\n",
       "    10.066085526965992,\n",
       "    0.07691213090851719,\n",
       "    13.38923196114951,\n",
       "    16.862422387837878,\n",
       "    21.382953923695233,\n",
       "    15.651918121909311,\n",
       "    14.440634953378058,\n",
       "    19.13130604146014,\n",
       "    22.114944705243296,\n",
       "    8.183429061888226,\n",
       "    13.699768012021506,\n",
       "    2.1212691930096144,\n",
       "    17.474216494453906,\n",
       "    7.8467696174922725,\n",
       "    2.6683841482907034,\n",
       "    0.11868201225906093,\n",
       "    9.064881467380093,\n",
       "    2.659801877718109,\n",
       "    4.055917032498944,\n",
       "    0.259848432909807,\n",
       "    0.413963629624058,\n",
       "    25.186704,\n",
       "    1.79722,\n",
       "    5.353545,\n",
       "    0.272499,\n",
       "    0.562898,\n",
       "    0.835397,\n",
       "    1.236854,\n",
       "    0.729917,\n",
       "    1.966771,\n",
       "    4.216321,\n",
       "    1.414081,\n",
       "    6.486208,\n",
       "    5.688314,\n",
       "    0.205632,\n",
       "    0.409204,\n",
       "    0.614836,\n",
       "    2.802168,\n",
       "    2.7549044689500004,\n",
       "    97.31541557350002,\n",
       "    0.069051,\n",
       "    0.151924,\n",
       "    0.130758,\n",
       "    0.06279,\n",
       "    0.027038,\n",
       "    0.999062,\n",
       "    0.096951,\n",
       "    0.042862,\n",
       "    0.096089,\n",
       "    0.100163,\n",
       "    1.033857,\n",
       "    1.034286,\n",
       "    0.016206,\n",
       "    0.00357,\n",
       "    0.016776,\n",
       "    1.488795,\n",
       "    0.915699,\n",
       "    0.232236,\n",
       "    0.012241,\n",
       "    0.074885,\n",
       "    0.131561,\n",
       "    0.096951,\n",
       "    0.004026,\n",
       "    0.009835,\n",
       "    0.011646,\n",
       "    0.250196,\n",
       "    0.131237,\n",
       "    0.768633,\n",
       "    0.015927,\n",
       "    0.539599,\n",
       "    0.451885,\n",
       "    0.001726,\n",
       "    0.003335,\n",
       "    0.001218,\n",
       "    1.236474,\n",
       "    0.000226,\n",
       "    0.555529,\n",
       "    0.000149,\n",
       "    0.001046,\n",
       "    0.002578,\n",
       "    0.126995,\n",
       "    0.732216,\n",
       "    0.037978,\n",
       "    0.019179,\n",
       "    0.720141,\n",
       "    0.018951,\n",
       "    0.013025,\n",
       "    0.059523,\n",
       "    0.027553,\n",
       "    0.000831,\n",
       "    0.0002,\n",
       "    0.073914,\n",
       "    0.061694,\n",
       "    0.002249,\n",
       "    0.007716,\n",
       "    0.236426,\n",
       "    0.0287,\n",
       "    0.05231,\n",
       "    0.041425,\n",
       "    0.033421,\n",
       "    0.017275,\n",
       "    0.001082,\n",
       "    0.011915,\n",
       "    0.004249,\n",
       "    0.196769,\n",
       "    0.039316,\n",
       "    0.038686,\n",
       "    0.00409,\n",
       "    0.003615,\n",
       "    0.116124,\n",
       "    0.051192,\n",
       "    0.025177,\n",
       "    0.0,\n",
       "    0.161908,\n",
       "    0.315775,\n",
       "    0.087229,\n",
       "    0.079586,\n",
       "    0.023227,\n",
       "    0.005966,\n",
       "    0.007901,\n",
       "    0.050376,\n",
       "    0.000186,\n",
       "    0.065723,\n",
       "    0.380193,\n",
       "    0.051566\n",
       "  ],\n",
       "  \"norm_std\": [\n",
       "    2.9210526350021033,\n",
       "    1.5294133532822065,\n",
       "    2.9209947673330334,\n",
       "    0.21956154740898992,\n",
       "    0.22097666681598954,\n",
       "    160.48566423804579,\n",
       "    151.38170855657367,\n",
       "    160.3304390667665,\n",
       "    60.484857692625106,\n",
       "    0.181038611279414,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.0,\n",
       "    0.24851193112366385,\n",
       "    0.317494124851492,\n",
       "    0.37175815103599535,\n",
       "    0.6098706561111424,\n",
       "    539.8195290502504,\n",
       "    8.140940922894863,\n",
       "    6.600767667198695,\n",
       "    6.700942921964325,\n",
       "    5.536318526756788,\n",
       "    4.020569431789569,\n",
       "    4.316039675035455,\n",
       "    3.229701298304296,\n",
       "    4.058753110098356,\n",
       "    2.399274478688092,\n",
       "    4.590084765547685,\n",
       "    1.8657465201411236,\n",
       "    8.197075845395899,\n",
       "    1.3989800795766576,\n",
       "    8.727770321711972,\n",
       "    4.719034225006412,\n",
       "    3.6844834579923407,\n",
       "    66.65125255607474,\n",
       "    11.022808176926917,\n",
       "    9.88512023443511,\n",
       "    5.895101555004671,\n",
       "    6.0315631910071374,\n",
       "    4.465786134186721,\n",
       "    8.73293454096314,\n",
       "    7.292192943139112,\n",
       "    5.798809757257198,\n",
       "    5.458840154330179,\n",
       "    5.34562222799046,\n",
       "    28.624753237838462,\n",
       "    22.7685485030176,\n",
       "    13.735506972569182,\n",
       "    12.75558914023291,\n",
       "    12.647297666063738,\n",
       "    16.73803715869515,\n",
       "    1.3236865505015507,\n",
       "    8.012917117258175,\n",
       "    6.328266302270954,\n",
       "    30.80439768300023,\n",
       "    14.510669158473307,\n",
       "    33.76748799216324,\n",
       "    0.0,\n",
       "    8.851153866015428,\n",
       "    8.222102882220607,\n",
       "    7.329351085680612,\n",
       "    4.87773057457412,\n",
       "    10.796349487508557,\n",
       "    24.55359833254403,\n",
       "    10.33295824604808,\n",
       "    8.986884190324291,\n",
       "    26.77991276665104,\n",
       "    29.521288543995215,\n",
       "    4.077418430037268,\n",
       "    11.23487898363004,\n",
       "    0.0,\n",
       "    50.277243284807206,\n",
       "    19.12173183245714,\n",
       "    9.819697177666312,\n",
       "    1.4201437981599128,\n",
       "    12.511435257208836,\n",
       "    14.212538029397628,\n",
       "    16.973978925056553,\n",
       "    19.21649041911615,\n",
       "    15.092240504961104,\n",
       "    19.889237093009676,\n",
       "    25.80872442073538,\n",
       "    9.254317550453825,\n",
       "    19.013243564373347,\n",
       "    3.6841568734614953,\n",
       "    17.690679185577395,\n",
       "    10.27595457263499,\n",
       "    3.3283202642652645,\n",
       "    2.8773795244438474,\n",
       "    9.228734822190495,\n",
       "    5.106296483962912,\n",
       "    4.008127533955226,\n",
       "    2.3345092198667503,\n",
       "    0.23958883840178574,\n",
       "    11.48532061063049,\n",
       "    2.0042680181777808,\n",
       "    3.411142707197923,\n",
       "    0.7103265443180337,\n",
       "    0.8009597262862117,\n",
       "    1.0630493791282618,\n",
       "    1.2495037990913607,\n",
       "    0.8592211073826755,\n",
       "    1.4909738617970663,\n",
       "    2.8049912821495706,\n",
       "    1.5692082041123125,\n",
       "    3.7188860712382157,\n",
       "    4.918753910447648,\n",
       "    0.6213838320183964,\n",
       "    0.6971589290933399,\n",
       "    0.9385507839118636,\n",
       "    1.7370945619837506,\n",
       "    2.7759468746763334,\n",
       "    43.91556441471313,\n",
       "    0.2929625321198007,\n",
       "    0.6742399816263887,\n",
       "    0.6447563579731193,\n",
       "    0.26136083143708466,\n",
       "    0.1703202147866646,\n",
       "    1.3696411924562566,\n",
       "    0.3394696140137124,\n",
       "    0.26977939457438505,\n",
       "    0.3350074869447194,\n",
       "    0.3408584597974497,\n",
       "    1.2690580420372088,\n",
       "    1.2684116362885036,\n",
       "    0.1297126917051003,\n",
       "    0.06304965563156611,\n",
       "    0.17914965229828922,\n",
       "    1.485673805113914,\n",
       "    1.1656052934139842,\n",
       "    0.5018632205797633,\n",
       "    0.15576643470973517,\n",
       "    0.2883562378800223,\n",
       "    0.3774901929558512,\n",
       "    0.3394696140137124,\n",
       "    0.07983606764988928,\n",
       "    0.10307416455777559,\n",
       "    0.11692041889415362,\n",
       "    1.0010868912132271,\n",
       "    0.7705779932112281,\n",
       "    1.157481598590082,\n",
       "    0.13507534533122212,\n",
       "    0.8359812306885952,\n",
       "    0.7600865243553028,\n",
       "    0.04757124327808961,\n",
       "    0.07183232513905516,\n",
       "    0.03513570421263404,\n",
       "    1.239225396368063,\n",
       "    0.015097985029438593,\n",
       "    1.3364349277900949,\n",
       "    0.013378265133341392,\n",
       "    0.032663541616103894,\n",
       "    0.060970137226002974,\n",
       "    0.44400840883756576,\n",
       "    1.159532265122051,\n",
       "    0.198246590935912,\n",
       "    0.1491817288215558,\n",
       "    1.28126795861232,\n",
       "    0.143114919141507,\n",
       "    0.11579880303510387,\n",
       "    0.25012811724209466,\n",
       "    0.1830406121462275,\n",
       "    0.03504726333553974,\n",
       "    0.015295758691880374,\n",
       "    0.3034514997274073,\n",
       "    0.2749689545601939,\n",
       "    0.04859983910409953,\n",
       "    0.09878498419533764,\n",
       "    0.5707110234042025,\n",
       "    0.17028898672063034,\n",
       "    0.24456026600763192,\n",
       "    0.21322057789532142,\n",
       "    0.1917343827305721,\n",
       "    0.13591391704896466,\n",
       "    0.03519702423260403,\n",
       "    0.11080182783711219,\n",
       "    0.0680510883818226,\n",
       "    0.5264724473438641,\n",
       "    0.2602735481879015,\n",
       "    0.25847912916802446,\n",
       "    0.10886360159063149,\n",
       "    0.10026934640727359,\n",
       "    0.35113436163289397,\n",
       "    0.2260341350934195,\n",
       "    0.16874580630684471,\n",
       "    0.0,\n",
       "    0.4146998571400424,\n",
       "    0.5347143492505464,\n",
       "    0.3137422508894841,\n",
       "    0.27962501103110715,\n",
       "    0.1547563582555832,\n",
       "    0.08130444916739461,\n",
       "    0.08949068223889126,\n",
       "    0.22530492534853602,\n",
       "    0.014421012861987593,\n",
       "    0.2736413019822887,\n",
       "    2.253629375384596,\n",
       "    0.22817317920167496\n",
       "  ],\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 3,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.37.2\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 600\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jointformer-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
