{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare molecule embeddings between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.utils.properties.smiles.physchem import PhysChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide parameters for running the script\n",
    "\n",
    "# Directory to save the outputs\n",
    "OUTPUT_DIR = 'embeddings_analysis_data/embeddings_analysis_output/'\n",
    "\n",
    "DATA_DIR = \"../../../../data\"\n",
    "\n",
    "# Paths to dataset and tokenizer configs\n",
    "PATH_TO_DATASET_CONFIG   = '../../configs/datasets/guacamol/physchem/'\n",
    "\n",
    "# Set list of properties to consider as labels\n",
    "PROPERTIES = ['MolLogP', 'TPSA', 'QED', 'MolWT']\n",
    "\n",
    "# If to take a sample of molecules for inference, can be None\n",
    "NUM_SAMPLES = 20000\n",
    "\n",
    "# Type of dimensionality reduction\n",
    "DIM_REDUCTION = 'pca'\n",
    "\n",
    "# Specify which dimensionalities of reduced embeddings to use for 2D plot\n",
    "# if \"first_two\" then first two dimensions are used. If \"top_correlated\", search for most correlated\n",
    "# dimensions with each property\n",
    "PERFORM_DIM_REDUCTION = True\n",
    "DIMENSIONS_FOR_VISUALIZATION = \"first_two\"\n",
    "CORR_TYPE = \"pearson\"\n",
    "\n",
    "FIGSIZE = (18, 8)\n",
    "SCATTERPLOT_KWARGS = {\n",
    "    'cmap': 'viridis',\n",
    "    'alpha': 0.6,\n",
    "}\n",
    "\n",
    "SEED = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "model_configs_dict = {\n",
    "\n",
    "    \"Jointformer\": {\n",
    "        \"path_to_tokenizer\": '../../../../checkpoints/jointformer/separate_task_token/configs/tokenizers/smiles_separate_task_token',\n",
    "        \"path_to_model_config\": '../../../../checkpoints/jointformer/separate_task_token/configs/models/jointformer_separate_task_token',\n",
    "        \"path_to_vocab\": \"../../data/vocabularies/deepchem.txt\",\n",
    "        \"path_to_model_checkpoint\": \"../../../../checkpoints/jointformer/separate_task_token/ckpt.pt\"\n",
    "    },\n",
    "\n",
    "    \"ChemBERTa\": {\n",
    "        \"path_to_tokenizer\": \"../../configs/tokenizers/chemberta\",\n",
    "        \"path_to_model_config\": '../../configs/models/chemberta_for_regression',\n",
    "        \"path_to_model_checkpoint\": \"DeepChem/ChemBERTa-77M-MTR\"\n",
    "    },\n",
    "\n",
    "    \"MolGPT\": {\n",
    "        \"path_to_tokenizer\": None,\n",
    "        \"path_to_model_config\": '../../configs/models/molgpt',\n",
    "        \"path_to_model_checkpoint\": \"../../../../checkpoints/molgpt/ckpt.pt\"\n",
    "\n",
    "    },\n",
    "\n",
    "    \"Unimol\": {\n",
    "        \"path_to_tokenizer\": None,\n",
    "        \"path_to_model_config\": '../../configs/models/unimol',\n",
    "        \"path_to_model_checkpoint\": \"../../../../checkpoints/unimol/ckpt.pt\"\n",
    "    },\n",
    "\n",
    "    \"MoLeR\": {\n",
    "        \"path_to_tokenizer\": None,\n",
    "        \"path_to_model_config\": '../../configs/models/moler',\n",
    "        \"path_to_model_checkpoint\": \"../../../../checkpoints/moler/ckpt.pkl\"\n",
    "    },\n",
    "    \n",
    "    # \"RegressionTransformer\": {\n",
    "    #     \"path_to_tokenizer\": None,\n",
    "    #     \"path_to_model_config\": '../../configs/models/regression_transformer',\n",
    "    #     \"path_to_model_checkpoint\": \"../../../../checkpoints/regression_transformer/logp_synthesizability/logp_and_synthesizability/pytorch_model.bin\"\n",
    "    # }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_configs_dict = {\n",
    "# \"RegressionTransformer\": {\n",
    "#         \"path_to_tokenizer\": None,\n",
    "#         \"path_to_model_config\": '../../configs/models/regression_transformer',\n",
    "#         \"path_to_model_checkpoint\": \"../../../../checkpoints/regression_transformer/logp_synthesizability/logp_and_synthesizability/pytorch_model.bin\"\n",
    "#     }\n",
    "\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(inputs, model, embedding_func, tokenizer, batch_size=32, **tokenizer_call_kwargs):\n",
    "    \"\"\"Compute embeddings in batches.\"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        inputs_batch = tokenizer(inputs[i:i + batch_size], **tokenizer_call_kwargs)\n",
    "        embeddings_batch = embedding_func(model, inputs_batch).detach()\n",
    "        embeddings.append(embeddings_batch)\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "def two_D_reduction(X, reducer=\"pca\", **reducer_kwargs):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction on the input data.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input data.\n",
    "        reducer (str, optional): The dimensionality reduction method to use. Options are 'pca' and 'tsne'. Defaults to 'pca'.\n",
    "        **reducer_kwargs: Additional keyword arguments to pass to the dimensionality reduction method.\n",
    "\n",
    "    Returns:\n",
    "        array-like: The reduced data.\n",
    "    \"\"\"\n",
    "    if reducer == \"pca\":\n",
    "        reducer = PCA(**reducer_kwargs)\n",
    "    elif reducer == \"tsne\":\n",
    "        reducer = TSNE(**reducer_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reducer: {reducer}\")\n",
    "\n",
    "    X_reduced = reducer.fit_transform(X)\n",
    "    return X_reduced\n",
    "\n",
    "def plot_2D_data_matplotlib(X_2d, ax=None, axis_titles=None, title=None, **scatter_kwargs):\n",
    "    \"\"\"\n",
    "    Plots the reduced data using matplotlib on the provided or a new axis.\n",
    "\n",
    "    Args:\n",
    "        X_2d (array-like): Reduced data.\n",
    "        ax (matplotlib.axes.Axes, optional): An existing axis to plot on. If None, a new figure and axis are created.\n",
    "        axis_aliases (list of str, optional): The aliases for the axes. Defaults to None.\n",
    "        **scatter_kwargs: Additional keyword arguments to pass to plt.scatter.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: The matplotlib axes containing the plot.\n",
    "    \"\"\"\n",
    "    # If no axis is provided, create a new figure and axis\n",
    "    p = ax.scatter(X_2d[:, 0], X_2d[:, 1], **scatter_kwargs)\n",
    "    if \"c\" in scatter_kwargs:\n",
    "        plt.colorbar(p, ax=ax)\n",
    "    ax.set_xlabel(axis_titles[0] if axis_titles is not None else \"\")\n",
    "    ax.set_ylabel(axis_titles[1] if axis_titles is not None else \"\")\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    return ax\n",
    "    \n",
    "def get_most_correlated_dimensions(X, y, method=\"pearson\", absolute_vals=True):\n",
    "    \"\"\"\n",
    "    Get the two most correlated dimensions of X with a reference vector y w.r.t. Pearson or Spearman correlation.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input data.\n",
    "        y (array-like): Reference vector.\n",
    "        method (str, optional): The correlation method to use. Options are 'pearson' and 'spearman'. Defaults to 'pearson'.\n",
    "        absolute_vals (bool, optional): Whether to consider the absolute values of the correlations. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The indices of the two most correlated dimensions.\n",
    "    \"\"\"\n",
    "    # Compute the correlation between each dimension of X and y\n",
    "    if method == \"pearson\":\n",
    "        correlations = np.array([pearsonr(X[:, i], y)[0] for i in range(X.shape[1])])\n",
    "    elif method == \"spearman\":\n",
    "        correlations = np.array([spearmanr(X[:, i], y)[0] for i in range(X.shape[1])])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown correlation method: {method}\")\n",
    "\n",
    "    # Get the indices of the two most correlated dimensions\n",
    "    if absolute_vals:\n",
    "        most_correlated_dims = np.argsort(np.abs(correlations))[::-1]\n",
    "    else:\n",
    "        most_correlated_dims = np.argsort(correlations)[::-1]\n",
    "\n",
    "    return most_correlated_dims, correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset to infer on\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='test')\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of property names\n",
    "phys_chem = PhysChem()\n",
    "property_names = phys_chem.descriptor_list\n",
    "\n",
    "# Get indexes of the properties to consider\n",
    "property_idx_dict = {prop: list(map(lambda x: x.lower(), property_names)).index(prop.lower()) for prop in PROPERTIES}\n",
    "\n",
    "# Get indexes of the properties to consider\n",
    "props, idxs = [], []\n",
    "for prop in PROPERTIES:\n",
    "    if prop.lower() not in list(map(lambda x: x.lower(), property_names)):\n",
    "        raise ValueError(f\"Property {prop} not found in the list of available properties.\")\n",
    "    idx = list(map(lambda x: x.lower(), property_names)).index(prop.lower()) \n",
    "    props.append(prop)\n",
    "    idxs.append(idx)\n",
    "    print(f\"User provided property name {prop} mapped to property name {property_names[idx]} with index {idx}.\")\n",
    "\n",
    "# Extract SMILES\n",
    "molecules_list = dataset.data\n",
    "\n",
    "# Extract proper labels corresponding to properties of choice\n",
    "labels = dataset.target[:, idxs]\n",
    "labels_df = pd.DataFrame(labels, columns=props)\n",
    "\n",
    "# Make sure you have correct labels data\n",
    "for prop in PROPERTIES:\n",
    "    df_values = labels_df[prop].values\n",
    "    idx = list(map(lambda x: x.lower(), property_names)).index(prop.lower())\n",
    "    assert property_names[idx].lower() == prop.lower(), f\"Property {prop} not found in the list of available properties.\"\n",
    "    assert np.allclose(df_values, dataset.target[:, idx]), f\"Property {prop} values do not match.\"\n",
    "\n",
    "# Optionally, take sample of the data\n",
    "if NUM_SAMPLES is not None:\n",
    "    # Sample indices\n",
    "    np.random.seed(SEED)\n",
    "    sample_indices = np.random.choice(len(molecules_list), NUM_SAMPLES, replace=False)\n",
    "    molecules_list = [molecules_list[i] for i in sample_indices]\n",
    "    labels_df = labels_df.iloc[sample_indices]\n",
    "\n",
    "assert len(molecules_list) == len(labels_df), \"Number of molecules and labels do not match.\"\n",
    "\n",
    "print()\n",
    "print(f\"Number of molecules to infer: {len(molecules_list)}\")\n",
    "print(f\"Number of properties: {len(labels_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save molecules and labels\n",
    "\n",
    "# Molecules list as txt\n",
    "with open(f'molecules_list_{NUM_SAMPLES}_{SEED}.txt', 'w') as f:\n",
    "    for item in molecules_list:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Labels as csv\n",
    "labels_df.to_csv(f'labels_df_{NUM_SAMPLES}_{SEED}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store embeddings\n",
    "embeddings_dict = {}\n",
    "\n",
    "for model_alias in model_configs_dict.keys():\n",
    "    print(f\"Loading model {model_alias}...\")\n",
    "    \n",
    "    # Get tokenizer if needed\n",
    "    if model_configs_dict[model_alias][\"path_to_tokenizer\"] is not None:\n",
    "        tokenizer_config = TokenizerConfig.from_config_file(model_configs_dict[model_alias][\"path_to_tokenizer\"])\n",
    "        if model_alias == \"Jointformer\":\n",
    "            tokenizer_config.path_to_vocabulary = model_configs_dict[model_alias][\"path_to_vocab\"]\n",
    "        tokenizer = AutoTokenizer.from_config(tokenizer_config)\n",
    "    else:\n",
    "        tokenizer = None\n",
    "    \n",
    "    # Get model\n",
    "    model_config = ModelConfig.from_config_file(model_configs_dict[model_alias][\"path_to_model_config\"])\n",
    "    model = AutoModel.from_config(model_config)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Load model checkpoint on cpu\n",
    "    model.eval()\n",
    "    model.to(device)    \n",
    "\n",
    "    if model_alias != \"ChemBERTa\":\n",
    "        try:\n",
    "            model.load_pretrained(model_configs_dict[model_alias][\"path_to_model_checkpoint\"])\n",
    "        except RuntimeError:\n",
    "            model.load_pretrained(model_configs_dict[model_alias][\"path_to_model_checkpoint\"], map_location='cpu')\n",
    "\n",
    "    # Get embeddings\n",
    "    print(f\"Computing embeddings for model {model_alias}...\")\n",
    "\n",
    "    smiles_encoder = model.to_smiles_encoder(tokenizer, batch_size=4, device=\"cpu\")\n",
    "    embeddings = smiles_encoder.encode(molecules_list)\n",
    "\n",
    "    # Store embeddings\n",
    "    embeddings_dict[model_alias] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in embeddings_dict.keys():\n",
    "    print(f\"Embeddings shape for {k}: {embeddings_dict[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save with pickle\n",
    "# import pickle\n",
    "\n",
    "# with open(f\"embeddings_dict_{NUM_SAMPLES}_molecules.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(embeddings_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings_dict_15000_molecules.pkl\", \"rb\") as f:\n",
    "    embeddings_dict = pickle.load(f)\n",
    "\n",
    "for k in embeddings_dict.keys():\n",
    "    print(f\"Embeddings shape for {k}: {embeddings_dict[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings - multiple targets\n",
    "if OUTPUT_DIR is not None and not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "NUM_COLS = len(PROPERTIES)\n",
    "NUM_ROWS = 2\n",
    "\n",
    "fig, axes = plt.subplots(NUM_ROWS, NUM_COLS, figsize=FIGSIZE)\n",
    "\n",
    "if DIM_REDUCTION == \"pca\":\n",
    "    axis_alias = 'PCA'\n",
    "elif DIM_REDUCTION == \"tsne\":\n",
    "    axis_alias = 'tSNE'\n",
    "\n",
    "# Iterate over properties\n",
    "for i, prop in enumerate(PROPERTIES):\n",
    "    labels = labels_df[prop].values\n",
    "\n",
    "    # Plot embeddings\n",
    "    # Jointformer\n",
    "    # Establish which dimensions to use for visualization\n",
    "    reduced_embeddings = embeddings_dict[\"Jointformer\"]\n",
    "    if DIMENSIONS_FOR_VISUALIZATION == \"first_two\":\n",
    "        current_2d_data = reduced_embeddings[:, :2]\n",
    "        ax1_alias = f\"{axis_alias} 1\"\n",
    "        ax2_alias = f\"{axis_alias} 2\"\n",
    "    elif DIMENSIONS_FOR_VISUALIZATION == \"top_correlated\":\n",
    "        most_correlated_dims = get_most_correlated_dimensions(reduced_embeddings, labels_df[prop].values, method=CORR_TYPE)\n",
    "        current_2d_data = reduced_embeddings[:, most_correlated_dims]\n",
    "        ax1_alias = f\"{axis_alias} {most_correlated_dims[0] + 1}\"\n",
    "        ax2_alias = f\"{axis_alias} {most_correlated_dims[1] + 1}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value for DIMENSIONS_FOR_VISUALIZATION: {DIMENSIONS_FOR_VISUALIZATION}\")\n",
    "    ax = axes[0, i]\n",
    "    plot_2D_data_matplotlib(current_2d_data, ax=ax, c=labels, \n",
    "                            title=f\"Jointformer\",\n",
    "                            **SCATTERPLOT_KWARGS)\n",
    "    ax.set_xlabel(ax1_alias)\n",
    "    ax.set_ylabel(ax2_alias)\n",
    "   \n",
    "    # Add colorbar title\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(prop)\n",
    "    \n",
    "    \n",
    "    # ChemBERTa\n",
    "    reduced_embeddings = embeddings_dict[\"ChemBERTa\"]\n",
    "    if DIMENSIONS_FOR_VISUALIZATION == \"first_two\":\n",
    "        current_2d_data = reduced_embeddings[:, :2]\n",
    "        ax1_alias = f\"{axis_alias} 1\"\n",
    "        ax2_alias = f\"{axis_alias} 2\"\n",
    "    elif DIMENSIONS_FOR_VISUALIZATION == \"top_correlated\":\n",
    "        most_correlated_dims = get_most_correlated_dimensions(reduced_embeddings, labels_df[prop].values, method=CORR_TYPE)\n",
    "        current_2d_data = reduced_embeddings[:, most_correlated_dims]\n",
    "        ax1_alias = f\"{axis_alias} {most_correlated_dims[0] + 1}\"\n",
    "        ax2_alias = f\"{axis_alias} {most_correlated_dims[1] + 1}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value for DIMENSIONS_FOR_VISUALIZATION: {DIMENSIONS_FOR_VISUALIZATION}\")\n",
    "    ax = axes[1, i]\n",
    "    plot_2D_data_matplotlib(current_2d_data, ax=ax, c=labels, \n",
    "                            title=f\"ChemBERTa\",\n",
    "                            **SCATTERPLOT_KWARGS)\n",
    "    ax.set_xlabel(ax1_alias)\n",
    "    ax.set_ylabel(ax2_alias)\n",
    "   \n",
    "    # Add colorbar title\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(prop)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top correlated dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for top correlated PCA analysis\n",
    "DIM_AGGREGATION = \"mean\"\n",
    "CORR_TYPE = \"pearson\"\n",
    "PERFORM_PCA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {}\n",
    "for model_alias in embeddings_dict:\n",
    "    embeddings = embeddings_dict[model_alias]\n",
    "    prop_dict = {}\n",
    "    if PERFORM_PCA:\n",
    "        pca = PCA(n_components=min(embeddings.shape[0], embeddings.shape[1]), random_state=SEED)\n",
    "        embeddings = pca.fit_transform(embeddings)\n",
    "    for prop in PROPERTIES:\n",
    "        # Top correlated dimensions\n",
    "        top_correlated_dims, correlations = get_most_correlated_dimensions(embeddings, labels_df[prop].values, method=CORR_TYPE)\n",
    "\n",
    "        if DIM_AGGREGATION == \"mean\":\n",
    "            score = np.mean(np.abs(correlations))\n",
    "            pval = None\n",
    "            prop_dict[prop] = score\n",
    "        elif DIM_AGGREGATION == \"max\":\n",
    "            score, pval = pearsonr(embeddings[:, top_correlated_dims[0]], labels_df[prop].values)\n",
    "            score = np.abs(score)\n",
    "            prop_dict[prop] = (score, pval)\n",
    "    res_dict[model_alias] = prop_dict\n",
    "\n",
    "res_df = pd.DataFrame(res_dict).T\n",
    "\n",
    "# Iterate over elements of df\n",
    "for i in range(res_df.shape[0]):\n",
    "    for j in range(res_df.shape[1]):\n",
    "        el = res_df.iloc[i, j]\n",
    "        if type(el) == tuple:\n",
    "            res_df.iloc[i, j] = (round(el[0], 3), round(el[1], 3))\n",
    "        else:\n",
    "            res_df.iloc[i, j] = round(el, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res_df.iloc[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return 1 - cosine(x, y)\n",
    "\n",
    "def euclidean_similarity(x, y):\n",
    "    return 1 / (1 + euclidean(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for distance plots\n",
    "NUM_PAIRS_PER_BIN = 3\n",
    "BIN_RANGE = (0.8, 1.0)\n",
    "NUM_BINS = 8\n",
    "SIMILAITY_METRIC = \"cosine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "\n",
    "if SIMILAITY_METRIC == \"cosine\":\n",
    "    similarity_func = cosine\n",
    "elif SIMILAITY_METRIC == \"euclidean\":\n",
    "    similarity_func = euclidean \n",
    "\n",
    "edges = np.linspace(BIN_RANGE[0], BIN_RANGE[1], num=NUM_BINS + 1)\n",
    "\n",
    "# Exclude the first and last values to make the range exclusive\n",
    "buckets = edges[1:-1]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, embeddings in embeddings_dict.items():\n",
    "    if model_name == \"Unimol\":\n",
    "        continue\n",
    "    per_bucket_dict = {}\n",
    "    for bucket_number, (lower_edge, upper_edge) in enumerate(zip(edges[:-1], edges[1:])):\n",
    "        per_bucket_dict[bucket_number] = {\n",
    "            \"similarities\": [],\n",
    "            \"prop_diffs\": {prop: [] for prop in PROPERTIES}\n",
    "        }\n",
    "        # Sample indices\n",
    "        num_tries = 0\n",
    "        num_matched_pairs = 0\n",
    "        while num_matched_pairs < NUM_PAIRS_PER_BIN:\n",
    "            idx1 = np.random.choice(len(molecules_list))\n",
    "            idx2 = np.random.choice(len(molecules_list))\n",
    "            num_tries += 1\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            # Calculate similarity\n",
    "            sim = similarity_func(embeddings[idx1], embeddings[idx2])\n",
    "            if sim > lower_edge and sim <= upper_edge:\n",
    "                num_matched_pairs += 1\n",
    "                # Store similarity\n",
    "                per_bucket_dict[bucket_number][\"similarities\"].append(sim)\n",
    "\n",
    "                for prop in PROPERTIES:\n",
    "                    prop_diff = np.abs(labels_df[prop].values[idx1] - labels_df[prop].values[idx2])\n",
    "                    per_bucket_dict[bucket_number][\"prop_diffs\"][prop].append(prop_diff)\n",
    "                if num_matched_pairs % 100 == 0:\n",
    "                    print(f\"Model {model_name}, bucket {bucket_number}, pairs {num_matched_pairs} done.\")\n",
    "\n",
    "        print(f\"Model {model_name}, bucket {bucket_number} done. Number of tries: {num_tries}\")\n",
    "\n",
    "    # Calculate mean and std for each bucket\n",
    "    mean_dict = {}\n",
    "    std_dict = {}\n",
    "\n",
    "    for bucket_number in per_bucket_dict:\n",
    "        mean_dict[bucket_number] = {\n",
    "            \"similarities\": np.mean(per_bucket_dict[bucket_number][\"similarities\"]),\n",
    "            \"prop_diffs\": {prop: np.mean(per_bucket_dict[bucket_number][\"prop_diffs\"][prop]) for prop in PROPERTIES}\n",
    "        }\n",
    "        std_dict[bucket_number] = {\n",
    "            \"similarities\": np.std(per_bucket_dict[bucket_number][\"similarities\"]),\n",
    "            \"prop_diffs\": {prop: np.std(per_bucket_dict[bucket_number][\"prop_diffs\"][prop]) for prop in PROPERTIES}\n",
    "        }\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"mean\": mean_dict,\n",
    "        \"std\": std_dict\n",
    "    }\n",
    "\n",
    "    print(f\"Model {model_name} done.\")\n",
    "\n",
    "# Now `results` contains the mean and std for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity vs. property difference for each property in separate plot\n",
    "# on every plot, show the models with different colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity vs. property difference for each property in separate plot\n",
    "fig, axes = plt.subplots(1, len(PROPERTIES), figsize=(18, 6))\n",
    "\n",
    "for i, prop in enumerate(PROPERTIES):\n",
    "    ax = axes[i]\n",
    "    similarities = [mean_dict[bucket_number][\"similarities\"] for bucket_number in mean_dict]\n",
    "    prop_diffs = [mean_dict[bucket_number][\"prop_diffs\"][prop] for bucket_number in mean_dict]\n",
    "    stds = [std_dict[bucket_number][\"prop_diffs\"][prop] for bucket_number in mean_dict]\n",
    "    ax.errorbar(similarities, prop_diffs, yerr=stds, fmt='o')\n",
    "    ax.set_xlabel(\"Similarity\")\n",
    "    ax.set_ylabel(f\"Mean {prop} difference\")\n",
    "    ax.set_title(f\"{prop} difference vs. similarity\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random pairs of molecules\n",
    "np.random.seed(SEED)\n",
    "pairs_indices = np.random.choice(len(molecules_list), NUM_PAIRS, replace=False)\n",
    "# Get tuples of indices\n",
    "pairs = [(pairs_indices[i], pairs_indices[i + 1]) for i in range(0, len(pairs_indices), 2)]\n",
    "\n",
    "jointformer_distances = []\n",
    "chemberta_distances = []\n",
    "properties_differences = {prop: [] for prop in PROPERTIES}\n",
    "\n",
    "for i, (idx1, idx2) in enumerate(pairs):\n",
    "    # Get embeddings\n",
    "    jointformer_emb1 = embeddings_dict[\"Jointformer\"][idx1]\n",
    "    jointformer_emb2 = embeddings_dict[\"Jointformer\"][idx2]\n",
    "    chemberta_emb1 = embeddings_dict[\"ChemBERTa\"][idx1]\n",
    "    chemberta_emb2 = embeddings_dict[\"ChemBERTa\"][idx2]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    jointformer_cosine = cosine(jointformer_emb1, jointformer_emb2)\n",
    "    chemberta_cosine = cosine(chemberta_emb1, chemberta_emb2)\n",
    "\n",
    "    jointformer_distances.append(jointformer_cosine)\n",
    "    chemberta_distances.append(chemberta_cosine)\n",
    "\n",
    "for prop in PROPERTIES:\n",
    "    for i, (idx1, idx2) in enumerate(pairs):\n",
    "        # Get property values\n",
    "        prop_val1 = labels_df[prop].values[idx1]\n",
    "        prop_val2 = labels_df[prop].values[idx2]\n",
    "        prop_diff = np.abs(prop_val1 - prop_val2)\n",
    "        properties_differences[prop].append(prop_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "fig, axes = plt.subplots(1, len(PROPERTIES), figsize=(15, 5))\n",
    "\n",
    "for i, prop in enumerate(PROPERTIES):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(jointformer_distances, properties_differences[prop], label=\"Jointformer\", alpha=0.6)\n",
    "    ax.scatter(chemberta_distances, properties_differences[prop], label=\"ChemBERTa\", alpha=0.6)\n",
    "    ax.set_xlabel(\"Cosine distance\")\n",
    "    ax.set_ylabel(f\"Absolute difference in {prop}\")\n",
    "    ax.set_title(f\"{prop}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
