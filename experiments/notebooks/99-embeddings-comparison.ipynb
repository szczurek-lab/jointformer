{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare molecule embeddings between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "from jointformer.configs.dataset import DatasetConfig\n",
    "from jointformer.configs.tokenizer import TokenizerConfig\n",
    "from jointformer.utils.datasets.auto import AutoDataset\n",
    "from jointformer.utils.tokenizers.auto import AutoTokenizer\n",
    "from jointformer.configs.model import ModelConfig\n",
    "from jointformer.models.auto import AutoModel\n",
    "from jointformer.utils.properties.smiles.physchem import PhysChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide parameters for running the script\n",
    "\n",
    "# Directory to save the outputs\n",
    "OUTPUT_DIR = 'embeddings_analysis_data/embeddings_analysis_output/'\n",
    "\n",
    "DATA_DIR = \"../../../../data\"\n",
    "\n",
    "# Paths to dataset and tokenizer configs\n",
    "PATH_TO_DATASET_CONFIG   = '../../configs/datasets/guacamol/physchem/'\n",
    "\n",
    "# Set list of properties to consider as labels\n",
    "PROPERTIES = ['MolLogP', 'TPSA', 'QED', 'MolWT']\n",
    "\n",
    "# If to take a sample of molecules for inference, can be None\n",
    "NUM_SAMPLES = 20000\n",
    "\n",
    "# Type of dimensionality reduction\n",
    "DIM_REDUCTION = 'pca'\n",
    "\n",
    "# Specify which dimensionalities of reduced embeddings to use for 2D plot\n",
    "# if \"first_two\" then first two dimensions are used. If \"top_correlated\", search for most correlated\n",
    "# dimensions with each property\n",
    "PERFORM_DIM_REDUCTION = True\n",
    "DIMENSIONS_FOR_VISUALIZATION = \"first_two\"\n",
    "CORR_TYPE = \"pearson\"\n",
    "\n",
    "FIGSIZE = (18, 8)\n",
    "SCATTERPLOT_KWARGS = {\n",
    "    'cmap': 'viridis',\n",
    "    'alpha': 0.6,\n",
    "}\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointformer parameters\n",
    "PATH_TO_JOINTFORMER_TOKENIZER_CONFIG = '../../configs/tokenizers/smiles'\n",
    "\n",
    "# Path to vocabulary file\n",
    "PATH_TO_JOINTFORMER_VOCAB = \"../../data/vocabularies/deepchem.txt\"\n",
    "\n",
    "# Path to model config\n",
    "PATH_TO_JOINTFORMER_MODEL_CONFIG = '../../configs/models/jointformer/'\n",
    "\n",
    "# Path to the pre-trained model checkpoint\n",
    "PATH_TO_JOINTFORMER_PRETRAINED_MODEL_CKPT = \"../../../../checkpoints/jointformer/no_separate_task_token/cls_embedding/05082024/ckpt.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemberta parameters\n",
    "PATH_TO_CHEMBERTA_MODEL_CONFIG = '../../configs//models/chemberta'\n",
    "PATH_TO_CHEMBERTA_TOKENIZER_CONFIG = \"../../configs/tokenizers/chemberta\"\n",
    "CHEMBERTA_CHECKPOINT = \"DeepChem/ChemBERTa-77M-MTR\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(inputs, model, embedding_func, tokenizer, batch_size=32, **tokenizer_call_kwargs):\n",
    "    \"\"\"Compute embeddings in batches.\"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        inputs_batch = tokenizer(inputs[i:i + batch_size], **tokenizer_call_kwargs)\n",
    "        embeddings_batch = embedding_func(model, inputs_batch).detach()\n",
    "        embeddings.append(embeddings_batch)\n",
    "    return torch.cat(embeddings)\n",
    "\n",
    "def two_D_reduction(X, reducer=\"pca\", **reducer_kwargs):\n",
    "    \"\"\"\n",
    "    Performs dimensionality reduction on the input data.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input data.\n",
    "        reducer (str, optional): The dimensionality reduction method to use. Options are 'pca' and 'tsne'. Defaults to 'pca'.\n",
    "        **reducer_kwargs: Additional keyword arguments to pass to the dimensionality reduction method.\n",
    "\n",
    "    Returns:\n",
    "        array-like: The reduced data.\n",
    "    \"\"\"\n",
    "    if reducer == \"pca\":\n",
    "        reducer = PCA(**reducer_kwargs)\n",
    "    elif reducer == \"tsne\":\n",
    "        reducer = TSNE(**reducer_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown reducer: {reducer}\")\n",
    "\n",
    "    X_reduced = reducer.fit_transform(X)\n",
    "    return X_reduced\n",
    "\n",
    "def plot_2D_data_matplotlib(X_2d, ax=None, axis_titles=None, title=None, **scatter_kwargs):\n",
    "    \"\"\"\n",
    "    Plots the reduced data using matplotlib on the provided or a new axis.\n",
    "\n",
    "    Args:\n",
    "        X_2d (array-like): Reduced data.\n",
    "        ax (matplotlib.axes.Axes, optional): An existing axis to plot on. If None, a new figure and axis are created.\n",
    "        axis_aliases (list of str, optional): The aliases for the axes. Defaults to None.\n",
    "        **scatter_kwargs: Additional keyword arguments to pass to plt.scatter.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.axes.Axes: The matplotlib axes containing the plot.\n",
    "    \"\"\"\n",
    "    # If no axis is provided, create a new figure and axis\n",
    "    p = ax.scatter(X_2d[:, 0], X_2d[:, 1], **scatter_kwargs)\n",
    "    if \"c\" in scatter_kwargs:\n",
    "        plt.colorbar(p, ax=ax)\n",
    "    ax.set_xlabel(axis_titles[0] if axis_titles is not None else \"\")\n",
    "    ax.set_ylabel(axis_titles[1] if axis_titles is not None else \"\")\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "\n",
    "    return ax\n",
    "    \n",
    "def get_most_correlated_dimensions(X, y, method=\"pearson\", absolute_vals=True):\n",
    "    \"\"\"\n",
    "    Get the two most correlated dimensions of X with a reference vector y w.r.t. Pearson or Spearman correlation.\n",
    "\n",
    "    Args:\n",
    "        X (array-like): Input data.\n",
    "        y (array-like): Reference vector.\n",
    "        method (str, optional): The correlation method to use. Options are 'pearson' and 'spearman'. Defaults to 'pearson'.\n",
    "        absolute_vals (bool, optional): Whether to consider the absolute values of the correlations. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The indices of the two most correlated dimensions.\n",
    "    \"\"\"\n",
    "    # Compute the correlation between each dimension of X and y\n",
    "    if method == \"pearson\":\n",
    "        correlations = np.array([pearsonr(X[:, i], y)[0] for i in range(X.shape[1])])\n",
    "    elif method == \"spearman\":\n",
    "        correlations = np.array([spearmanr(X[:, i], y)[0] for i in range(X.shape[1])])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown correlation method: {method}\")\n",
    "\n",
    "    # Get the indices of the two most correlated dimensions\n",
    "    if absolute_vals:\n",
    "        most_correlated_dims = np.argsort(np.abs(correlations))[::-1][:2]\n",
    "    else:\n",
    "        most_correlated_dims = np.argsort(correlations)[::-1][:2]\n",
    "\n",
    "    return most_correlated_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset to infer on\n",
    "dataset_config = DatasetConfig.from_config_file(PATH_TO_DATASET_CONFIG)\n",
    "dataset = AutoDataset.from_config(dataset_config, data_dir=DATA_DIR, split='test')\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of property names\n",
    "phys_chem = PhysChem()\n",
    "property_names = phys_chem.descriptor_list\n",
    "\n",
    "# Get indexes of the properties to consider\n",
    "property_idx_dict = {prop: list(map(lambda x: x.lower(), property_names)).index(prop.lower()) for prop in PROPERTIES}\n",
    "\n",
    "# Get indexes of the properties to consider\n",
    "props, idxs = [], []\n",
    "for prop in PROPERTIES:\n",
    "    if prop.lower() not in list(map(lambda x: x.lower(), property_names)):\n",
    "        raise ValueError(f\"Property {prop} not found in the list of available properties.\")\n",
    "    idx = list(map(lambda x: x.lower(), property_names)).index(prop.lower()) \n",
    "    props.append(prop)\n",
    "    idxs.append(idx)\n",
    "    print(f\"User provided property name {prop} mapped to property name {property_names[idx]} with index {idx}.\")\n",
    "\n",
    "# Extract SMILES\n",
    "molecules_list = dataset.data\n",
    "\n",
    "# Extract proper labels corresponding to properties of choice\n",
    "labels = dataset.target[:, idxs]\n",
    "labels_df = pd.DataFrame(labels, columns=props)\n",
    "\n",
    "# Make sure you have correct labels data\n",
    "for prop in PROPERTIES:\n",
    "    df_values = labels_df[prop].values\n",
    "    idx = list(map(lambda x: x.lower(), property_names)).index(prop.lower())\n",
    "    assert property_names[idx].lower() == prop.lower(), f\"Property {prop} not found in the list of available properties.\"\n",
    "    assert np.allclose(df_values, dataset.target[:, idx]), f\"Property {prop} values do not match.\"\n",
    "\n",
    "# Optionally, take sample of the data\n",
    "if NUM_SAMPLES is not None:\n",
    "    # Sample indices\n",
    "    np.random.seed(SEED)\n",
    "    sample_indices = np.random.choice(len(molecules_list), NUM_SAMPLES, replace=False)\n",
    "    molecules_list = [molecules_list[i] for i in sample_indices]\n",
    "    labels_df = labels_df.iloc[sample_indices]\n",
    "\n",
    "assert len(molecules_list) == len(labels_df), \"Number of molecules and labels do not match.\"\n",
    "\n",
    "print()\n",
    "print(f\"Number of molecules to infer: {len(molecules_list)}\")\n",
    "print(f\"Number of properties: {len(labels_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store embeddings\n",
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jointformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenizer\n",
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_JOINTFORMER_TOKENIZER_CONFIG)\n",
    "tokenizer_config.path_to_vocabulary = PATH_TO_JOINTFORMER_VOCAB\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)\n",
    "\n",
    "\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_JOINTFORMER_MODEL_CONFIG)\n",
    "model = AutoModel.from_config(model_config)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load weights\n",
    "model.load_pretrained(PATH_TO_JOINTFORMER_PRETRAINED_MODEL_CKPT)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Compute embeddings\n",
    "smiles_encoder = model.to_smiles_encoder(tokenizer, batch_size=4, device=\"cpu\")\n",
    "\n",
    "embeddings = smiles_encoder.encode(molecules_list)\n",
    "\n",
    "# Store embeddings\n",
    "embeddings_dict[\"Jointformer\"] = embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_config = TokenizerConfig.from_config_file(PATH_TO_CHEMBERTA_TOKENIZER_CONFIG)\n",
    "tokenizer = AutoTokenizer.from_config(tokenizer_config)\n",
    "\n",
    "# Get model\n",
    "model_config = ModelConfig.from_config_file(PATH_TO_CHEMBERTA_MODEL_CONFIG)\n",
    "model_config.pretrained_filepath = CHEMBERTA_CHECKPOINT\n",
    "model = AutoModel.from_config(model_config)\n",
    "\n",
    "smiles_encoder = model.to_smiles_encoder(tokenizer, batch_size=4, device='cpu')\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = smiles_encoder.encode(molecules_list)\n",
    "\n",
    "# Reduce dimensionality\n",
    "if PERFORM_DIM_REDUCTION:\n",
    "    reduced_embeddings = two_D_reduction(embeddings, reducer=DIM_REDUCTION, n_components=embeddings.shape[1],\n",
    "                                         random_state=SEED)\n",
    "else:\n",
    "    reduced_embeddings = embeddings\n",
    "\n",
    "embeddings_dict[\"ChemBERTa\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in embeddings_dict.keys():\n",
    "    print(f\"Embeddings shape for {k}: {embeddings_dict[k].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings - multiple targets\n",
    "if OUTPUT_DIR is not None and not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "NUM_COLS = len(PROPERTIES)\n",
    "NUM_ROWS = 2\n",
    "\n",
    "fig, axes = plt.subplots(NUM_ROWS, NUM_COLS, figsize=FIGSIZE)\n",
    "\n",
    "if DIM_REDUCTION == \"pca\":\n",
    "    axis_alias = 'PCA'\n",
    "elif DIM_REDUCTION == \"tsne\":\n",
    "    axis_alias = 'tSNE'\n",
    "\n",
    "# Iterate over properties\n",
    "for i, prop in enumerate(PROPERTIES):\n",
    "    labels = labels_df[prop].values\n",
    "\n",
    "    # Plot embeddings\n",
    "    # Jointformer\n",
    "    # Establish which dimensions to use for visualization\n",
    "    reduced_embeddings = embeddings_dict[\"Jointformer\"]\n",
    "    if DIMENSIONS_FOR_VISUALIZATION == \"first_two\":\n",
    "        current_2d_data = reduced_embeddings[:, :2]\n",
    "        ax1_alias = f\"{axis_alias} 1\"\n",
    "        ax2_alias = f\"{axis_alias} 2\"\n",
    "    elif DIMENSIONS_FOR_VISUALIZATION == \"top_correlated\":\n",
    "        most_correlated_dims = get_most_correlated_dimensions(reduced_embeddings, labels_df[prop].values, method=CORR_TYPE)\n",
    "        current_2d_data = reduced_embeddings[:, most_correlated_dims]\n",
    "        ax1_alias = f\"{axis_alias} {most_correlated_dims[0] + 1}\"\n",
    "        ax2_alias = f\"{axis_alias} {most_correlated_dims[1] + 1}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value for DIMENSIONS_FOR_VISUALIZATION: {DIMENSIONS_FOR_VISUALIZATION}\")\n",
    "    ax = axes[0, i]\n",
    "    plot_2D_data_matplotlib(current_2d_data, ax=ax, c=labels, \n",
    "                            title=f\"Jointformer\",\n",
    "                            **SCATTERPLOT_KWARGS)\n",
    "    ax.set_xlabel(ax1_alias)\n",
    "    ax.set_ylabel(ax2_alias)\n",
    "   \n",
    "    # Add colorbar title\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(prop)\n",
    "    \n",
    "    \n",
    "    # ChemBERTa\n",
    "    reduced_embeddings = embeddings_dict[\"ChemBERTa\"]\n",
    "    if DIMENSIONS_FOR_VISUALIZATION == \"first_two\":\n",
    "        current_2d_data = reduced_embeddings[:, :2]\n",
    "        ax1_alias = f\"{axis_alias} 1\"\n",
    "        ax2_alias = f\"{axis_alias} 2\"\n",
    "    elif DIMENSIONS_FOR_VISUALIZATION == \"top_correlated\":\n",
    "        most_correlated_dims = get_most_correlated_dimensions(reduced_embeddings, labels_df[prop].values, method=CORR_TYPE)\n",
    "        current_2d_data = reduced_embeddings[:, most_correlated_dims]\n",
    "        ax1_alias = f\"{axis_alias} {most_correlated_dims[0] + 1}\"\n",
    "        ax2_alias = f\"{axis_alias} {most_correlated_dims[1] + 1}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value for DIMENSIONS_FOR_VISUALIZATION: {DIMENSIONS_FOR_VISUALIZATION}\")\n",
    "    ax = axes[1, i]\n",
    "    plot_2D_data_matplotlib(current_2d_data, ax=ax, c=labels, \n",
    "                            title=f\"ChemBERTa\",\n",
    "                            **SCATTERPLOT_KWARGS)\n",
    "    ax.set_xlabel(ax1_alias)\n",
    "    ax.set_ylabel(ax2_alias)\n",
    "   \n",
    "    # Add colorbar title\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(prop)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine, euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PAIRS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random pairs of molecules\n",
    "np.random.seed(SEED)\n",
    "pairs_indices = np.random.choice(len(molecules_list), NUM_PAIRS, replace=False)\n",
    "# Get tuples of indices\n",
    "pairs = [(pairs_indices[i], pairs_indices[i + 1]) for i in range(0, len(pairs_indices), 2)]\n",
    "\n",
    "jointformer_distances = []\n",
    "chemberta_distances = []\n",
    "properties_differences = {prop: [] for prop in PROPERTIES}\n",
    "\n",
    "for i, (idx1, idx2) in enumerate(pairs):\n",
    "    # Get embeddings\n",
    "    jointformer_emb1 = embeddings_dict[\"Jointformer\"][idx1]\n",
    "    jointformer_emb2 = embeddings_dict[\"Jointformer\"][idx2]\n",
    "    chemberta_emb1 = embeddings_dict[\"ChemBERTa\"][idx1]\n",
    "    chemberta_emb2 = embeddings_dict[\"ChemBERTa\"][idx2]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    jointformer_cosine = cosine(jointformer_emb1, jointformer_emb2)\n",
    "    chemberta_cosine = cosine(chemberta_emb1, chemberta_emb2)\n",
    "\n",
    "    jointformer_distances.append(jointformer_cosine)\n",
    "    chemberta_distances.append(chemberta_cosine)\n",
    "\n",
    "for prop in PROPERTIES:\n",
    "    for i, (idx1, idx2) in enumerate(pairs):\n",
    "        # Get property values\n",
    "        prop_val1 = labels_df[prop].values[idx1]\n",
    "        prop_val2 = labels_df[prop].values[idx2]\n",
    "        prop_diff = np.abs(prop_val1 - prop_val2)\n",
    "        properties_differences[prop].append(prop_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "fig, axes = plt.subplots(1, len(PROPERTIES), figsize=(15, 5))\n",
    "\n",
    "for i, prop in enumerate(PROPERTIES):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(jointformer_distances, properties_differences[prop], label=\"Jointformer\", alpha=0.6)\n",
    "    ax.scatter(chemberta_distances, properties_differences[prop], label=\"ChemBERTa\", alpha=0.6)\n",
    "    ax.set_xlabel(\"Cosine distance\")\n",
    "    ax.set_ylabel(f\"Absolute difference in {prop}\")\n",
    "    ax.set_title(f\"{prop}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
